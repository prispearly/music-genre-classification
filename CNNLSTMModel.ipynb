{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7e8fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16512/2835146953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLearningRateMonitor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloggers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensorBoardLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from models.utils import Normalization\n",
    "import fastwer\n",
    "import contextlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38eeb2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: conda-script.py [-h] [-V] command ...\n",
      "conda-script.py: error: unrecognized arguments: pytorch-lightning\n"
     ]
    }
   ],
   "source": [
    "class CNN_biLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 spec_layer,\n",
    "                 norm_mode,\n",
    "                 input_dim,\n",
    "                 hidden_dim=1024,\n",
    "                 output_dim=88):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.spec_layer = spec_layer\n",
    "        self.norm_layer = Normalization(mode=norm_mode)\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            # layer 0\n",
    "            nn.Conv2d(1, hidden_dim // 4, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            # layer 1\n",
    "            nn.Conv2d(hidden_dim // 4, hidden_dim // 4, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            # layer 2\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(hidden_dim // 4, hidden_dim // 2, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            # layer 3\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear((hidden_dim // 2) * (input_dim // 4), hidden_dim),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.bilstm = nn.LSTM(hidden_dim, hidden_dim//2, batch_first=True, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        spec = self.spec_layer(x) # (B, F, T)\n",
    "        spec = torch.log(spec+1e-8)\n",
    "        spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec = self.norm_layer(spec)\n",
    "        spec = spec.unsqueeze(1) # (B, 1, T, F)\n",
    "\n",
    "        x = self.cnn(spec) # (B, hidden_dim//8, T, F//4)\n",
    "        x = x.transpose(1,2).flatten(2)\n",
    "        x = self.fc(x) # (B, T, hidden_dim//8*F//4)\n",
    "        x, _ = self.bilstm(x)\n",
    "        \n",
    "        pred = self.classifier(x)\n",
    "        \n",
    "        \n",
    "        return pred       \n",
    "        \n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 spec_layer,\n",
    "                 norm_mode,\n",
    "                 input_dim,\n",
    "                 hidden_dim=1024,\n",
    "                 output_dim=88):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.spec_layer = spec_layer\n",
    "        self.norm_layer = Normalization(mode=norm_mode)\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            # layer 0\n",
    "            nn.Conv2d(1, hidden_dim // 4, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            # layer 1\n",
    "            nn.Conv2d(hidden_dim // 4, hidden_dim // 4, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            # layer 2\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(hidden_dim // 4, hidden_dim // 2, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            # layer 3\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear((hidden_dim // 2) * (input_dim // 4), hidden_dim),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.bilstm = nn.LSTM(hidden_dim, hidden_dim//2, batch_first=True, num_layers=1, bidirectional=False)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim //2 , output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        spec = self.spec_layer(x) # (B, F, T)\n",
    "        spec = torch.log(spec+1e-8)\n",
    "        spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec = self.norm_layer(spec)\n",
    "        spec = spec.unsqueeze(1) # (B, 1, T, F)\n",
    "\n",
    "        x = self.cnn(spec) # (B, hidden_dim//8, T, F//4)\n",
    "        x = x.transpose(1,2).flatten(2)\n",
    "        x = self.fc(x) # (B, T, hidden_dim//8*F//4)\n",
    "        x, _ = self.bilstm(x)\n",
    "        \n",
    "        pred = self.classifier(x)\n",
    "        \n",
    "        return pred       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03150cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN THE MODEL\n",
    "model = CNN_biLSTM(spec_layer, norm_mode, input_dim = 512, hidden_dim = 1024, output_dim =88 )\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnAudio import Spectrogram\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from utils.text_processing import TextTransform, data_processing\n",
    "\n",
    "# For loading the output class ddictionary\n",
    "import pickle\n",
    "# Loading dataset\n",
    "#TODO: train_dataset = \n",
    "#TODO: test_dataset =\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [190, 20], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "   \n",
    "#TODO: SpecLayer = getattr(Spectrogram)\n",
    "\n",
    "#TODO: trainer = pl.Trainer() create a trainer, should have argument inside\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "torch.manual_seed(28)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fac3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, data):\n",
    "    \n",
    "    # Compute accuracy\n",
    "    \n",
    "            \n",
    "    return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for context, target in data:\n",
    "        \n",
    "        # Prepare data :Spectrogram \n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        model.zero_grad()\n",
    "        output = model(ids)\n",
    "        # Reshape to cover for absence of minibatches (needed for loss function)\n",
    "        \n",
    "        loss = loss_func(output, target)\n",
    "        \n",
    "        # Backward pass and optim\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Loss update\n",
    "        total_loss += loss.data.item()\n",
    "    \n",
    "    # Display\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = check_accuracy(model, data)\n",
    "        print(\"Accuracy after epoch {} is {}\".format(epoch, accuracy))\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
